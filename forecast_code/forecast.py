# -*- coding: utf-8 -*-
"""Copy of Report_On_Enhancing_CFSv2_Temperature_Forecasts2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AeSIk6gAoi5jNOgD71LIpPJKG3zr8XsN
"""

# Core Libraries
import pandas as pd  # Data manipulation and analysis
import numpy as np   # Numerical computations

# Machine Learning
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Deep Learning
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten, LeakyReLU
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Cloud Storage and Data Handling
from google.cloud import storage
import pyarrow.parquet as pq
import pyarrow as pa
import gcsfs

# Utilities
from io import BytesIO
import requests
from tqdm import tqdm
import concurrent.futures
import datetime

n_forecast = 180
n_member = 5

## Gather VERA Observation Data of AirTemp_C_mean for FCRE Area
url = "https://renc.osn.xsede.org/bio230121-bucket01/vera4cast/targets/project_id=vera4cast/duration=PT1H/hourly-met-targets.csv.gz"
vera_df = pd.read_csv(url, compression='gzip')
observations_df = vera_df[vera_df['variable'] == 'AirTemp_C_mean']
observations_df.head(5)

## Format for N days in Advance for Training / Testing Output Sets
## Date: t
## Observation for a given date: observation(t)
## Pull:
## observation(t+1), observation(t+2), ..., observation(t+n_forecast)

observations_df = observations_df.copy()
observations_df['date'] = pd.to_datetime(observations_df['datetime']).dt.date
daily_mean = observations_df.groupby('date')['observation'].mean().reset_index()
mean_out = daily_mean.copy()

for i in range(1, n_forecast + 1):
    mean_out[f'observation(t+{i})'] = mean_out['observation'].shift(-i)

mean_out.dropna(inplace=True)
mean_out.reset_index(drop=True, inplace=True)
mean_out = mean_out.drop(['observation'], axis=1)

## Set Date and datetime format index
mean_out['date'] = pd.to_datetime(mean_out['date'])
mean_out.set_index('date', inplace=True)

from google.colab import auth
auth.authenticate_user()
from datetime import timedelta
import io
from google.cloud import storage

# Initialize Google Cloud client
client = storage.Client()

# Shared bucket name
BUCKET_NAME = 'adiabat-cmda'

# Initialize GCSFileSystem
fs = gcsfs.GCSFileSystem()

# Function to process each file
def process_file(file):

    # Extract reference datetime from the filename
    ref_date_time_str = file.split('.')[0][-8:]  # Assumes last 8 chars represent date in 'YYYYMMDD' format
    ref_date_time = pd.to_datetime(ref_date_time_str, format='%Y%m%d')

    # Define min and max datetime for filtering
    min_date = (ref_date_time + timedelta(days=1))
    max_date = (ref_date_time + timedelta(days=n_forecast))

    # Step 1: Use a rough filter to narrow down rows by reference_datetime date
    filters = [
        # Assuming midnight reference
        ('reference_datetime', '=', ref_date_time.strftime('%Y-%m-%d 00:00')),
        ('datetime', '>=', min_date.strftime('%Y-%m-%d 00:00')),
        ('datetime', '<=', max_date.strftime('%Y-%m-%d 23:00'))
    ]

    return pd.read_parquet(io.BytesIO(fs.cat(f'{BUCKET_NAME}/{file}')), filters=filters)

# List all Parquet files across the entire bucket
parquet_files = [blob.name for blob in client.list_blobs(BUCKET_NAME) if blob.name.endswith('.parquet')]

filtered_tables = [None] * len(parquet_files)


def process_and_store_file(index, file_name):
    try:
        result = process_file(file_name)  # Assuming process_file correctly returns a DataFrame or filtered data
        return index, result
    except Exception as e:
        print(f"Error processing {file_name}: {e}")
        return index, None

with concurrent.futures.ThreadPoolExecutor() as executor:
    # Map each file and index to the executor
    futures = {executor.submit(process_and_store_file, i, file): i for i, file in enumerate(parquet_files)}

    # Use tqdm to show a progress bar for the number of files being processed
    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="Processing Files"):
        index, result = future.result()
        filtered_tables[index] = result

# Flatten filtered_tables to remove any None entries, if needed
filtered_tables = [table for table in filtered_tables if table is not None]

# Combine all individual DataFrames into one, if each result is a DataFrame
predictions_df = pd.concat(filtered_tables, ignore_index=True)
# reference date time object
predictions_df['reference_datetime'] = pd.to_datetime(predictions_df['reference_datetime'])
predictions_df['datetime'] = pd.to_datetime(predictions_df['datetime'])

## Organize data in following format
## reference_datetime | t+1 ensemble1 | t+1 ensemble1 | ... | t+n ensemble4

predictions_df['time_delta_days'] = (predictions_df['datetime'] - predictions_df['reference_datetime']).dt.days

merged_df = predictions_df.groupby(['reference_datetime', 'time_delta_days']).agg({'prediction': 'mean'}).reset_index()

merged_df = merged_df.pivot_table(
    index='reference_datetime',
    columns='time_delta_days',
    values='prediction',
    aggfunc='mean'
)


merged_df.columns = [f"t+{day} mu" for day in merged_df.columns]
merged_df = merged_df.reset_index()


merged_df['reference_datetime'] = pd.to_datetime(merged_df['reference_datetime'])
merged_df.set_index('reference_datetime', inplace=True)

mean_out.index = pd.to_datetime(mean_out.index)
mean_out_shifted = mean_out.copy()

mean_out_shifted.index = mean_out_shifted.index + pd.Timedelta(days=n_forecast)

merged_df = pd.merge(merged_df, mean_out_shifted, left_on='reference_datetime', right_index=True, how='left')

# Split into training and test sets (80-20 split)
split_index = int(len(merged_df) * 0.8)
train_df = merged_df.iloc[:split_index]
test_df = merged_df.iloc[split_index:]

from datetime import datetime, timedelta

train_start = "2018-11-02"

train_end = "2023-05-01"

ts_date = datetime.strptime(train_end, "%Y-%m-%d")
te_date = ts_date + timedelta(days=n_forecast)
test_start = te_date.strftime("%Y-%m-%d")

test_start_date = datetime.strptime(test_start, "%Y-%m-%d")
test_end_date = test_start_date + timedelta(days=n_forecast)
test_end = test_end_date.strftime("%Y-%m-%d")

x_train = merged_df.loc[train_start:train_end].dropna().values
y_train = mean_out.loc[train_start:train_end].dropna().values
x_test = merged_df.loc[test_start:test_end].dropna().values
y_test = mean_out.loc[test_start:test_end].dropna().values

early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
early_stopping_val = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

def create_model(x):
    model = Sequential([
        Dense(512, activation='relu', input_shape=(x.shape[1],)),
        LeakyReLU(alpha=0.1),
        Dense(256, activation='relu'),
        LeakyReLU(alpha=0.1),
        Dense(128, activation='relu'),
        LeakyReLU(alpha=0.1),
        Dense(64, activation='relu'),
        LeakyReLU(alpha=0.1),
        Dense(n_forecast)
    ])
    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mae')
    return model

# train on full training set
ensemble = [create_model(x_train) for _ in range(5)]
for i, model in enumerate(ensemble):
    model.fit(x_train, y_train, epochs=200, batch_size=16, validation_split=0.2, callbacks=[early_stopping], verbose=0)

predictions = np.array([model.predict(x_test) for model in ensemble])

next = merged_df.loc[mean_out.index.max()]
allnextmonth = np.array([])
for member in ensemble:
    allnextmonth = np.append(allnextmonth, member.predict(next.values.reshape(1, -1)))

forecast = pd.DataFrame({'project_id': ['vera4cast'] * n_forecast * n_member,
              'model_id': ['churner'] * n_forecast * n_member,
              'datetime': list(pd.date_range(start=mean_out.index.max()+timedelta(days=1),
                                        end=mean_out.index.max()+timedelta(days=n_forecast))) * n_member,
              'reference_datetime': [str(mean_out.index.max().date())] * n_forecast * n_member,
              'duration': ['P1D'] * n_forecast * n_member,
              'site_id': ['fcre'] * n_forecast * n_member,
              'depth_m': [1.6] * n_forecast * n_member,
              'family': ['ensemble'] * n_forecast * n_member,
              'parameter': np.repeat(np.arange(5) + 1, n_forecast),
              'variable': ['Temp_C_mean'] * n_forecast * n_member,
              'prediction': allnextmonth
              })

